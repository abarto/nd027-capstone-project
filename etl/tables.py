"""
Fact and dimension tabales related functions for "Project: Capstone Project"
"""
from pyspark.sql import functions as F

from .common import _simple_validation


def _get_dim_country_df(
    countries_df,
    languages_df,
    national_accounts_df
):
    """
    Create a PySpark DataFrame to be loaded as dimCountry
    
    | Column         | Type      | PK | Description                       |
    |----------------|-----------|----|-----------------------------------|
    | country_id     | char      | Y  | The ISO identifier of the country |
    | name           | varchar   |    | Name of the country               |
    | languages      | varchar[] |    | A list of languages spoken        |
    | gdp_per_capita | double    |    | GDP per capita                    |
    """
    dim_country_df = countries_df.select(["alpha3", "name", "languages"])\
        .withColumnRenamed("alpha3", "country_id")\
        .withColumn("languages", F.explode(F.col("languages")))\
        .join(
            languages_df,
            F.col("languages") == languages_df.alpha3,
            "left"
        ).select(
            "country_id",
            countries_df.name,
            languages_df.name.alias("languages")
        ).groupBy("country_id").agg(
            F.first("name").alias("name"),
            F.collect_list("languages").alias("languages")
        ).join(
            national_accounts_df,
            F.lower(F.col("name")) == F.lower(national_accounts_df.country),
            "left"
        ).withColumn("gdp_per_capita", F.coalesce("gdp_per_capita", F.lit(0)))\
        .drop("country")

    return dim_country_df


def _validate_dim_country_df(df):
    """Performs simple validations on the dimCountry DataFrame"""
    
    _simple_validation(df, "dim_country_df", ["country_id", "nane", "languages", "gdp_per_capita"], 100)


def generate_dim_country_parquet(
    countries_df,
    languages_df,
    national_accounts_df,
    output_dir="parquet_files"
):
    """Generates dimCountry parquet files"""
    dim_country_df = _get_dim_country_df(
        countries_df,
        languages_df,
        national_accounts_df
    )
    _validate_dim_country_df(dim_country_df)
    dim_country_df.write.mode("overwrite").parquet(f"{output_dir}/dim_country")


def _get_dim_airport_df(airports_df):
    """
    Create a PySpark DataFrame to be loaded as dimAirport

    | Column       | Type      | PK | Description                        |
    |--------------|-----------|:--:|------------------------------------|
    | airport_id   | char      | Y  | The 4 character airport identifier |
    | name         | varchar   |    | Name of the airport                |
    | municipality | varchar   |    | Municipality/city                  |
    | state        | char[2]   |    | State                              |
    """
    return airports_df.withColumnRenamed("ident", "airport_id")


def _validate_dim_airport_df(df):
    """Performs simple validations on the dimAirport DataFrame"""
    
    _simple_validation(df, "dim_airport_df", ["airport_id", "nane", "municipality", "state"], 100)


def generate_dim_airport_parquet(
    airports_df,
    output_dir="parquet_files"
):
    """Generates dimCountry parquet files"""
    dim_airport_df = _get_dim_airport_df(
        airports_df
    )
    _validate_dim_airport_df(dim_airport_df)
    dim_airport_df.write.mode("overwrite").parquet(f"{output_dir}/dim_airport")


def _get_fact_ingress_df(
    i94_data_df,
    airports_df,
    cbp_codes_df,
    countries_df,
    i94cntyl_df,
    temperatures_df,
    output_dir="parquet_files"
):
    """
    Create a PySpark DataFrame to be loaded as factIngress

    | Column             | Type    | PK | Description                                        |
    |--------------------|---------|:--:|----------------------------------------------------|
    | id                 | int     | Y  | Unique autogenerated identifier of the fact record |
    | date_id            | int     |    | Date of ingress                                    |
    | year               | int     |    | Year of ingress (used for partitioning)            |
    | country_id         | char[2] |    | Country of origin                                  |
    | airport_id         | char[4] |    | Airport                                            |
    | gender             | char    |    | Gender.                                            |
    | age_bucket         | char[2] |    | Age bucket.                                        |
    | temperature_bucket | char[2] |    | Average weekly temperature bucket.                 |
    """
    fact_ingress_df = i94_data_df.join(
        cbp_codes_df,
        F.col("i94port") == cbp_codes_df.code
    ).join(
        airports_df,
        (cbp_codes_df.state == airports_df.state) & (cbp_codes_df.municipality == airports_df.municipality) 
    ).join(
        temperatures_df,
        (F.lower(cbp_codes_df.municipality) == F.lower(temperatures_df.city))
        & (F.weekofyear(F.col("arrdate")) == temperatures_df.week_of_year),
        "left"
    ).withColumn("temperature_bucket", F.coalesce(temperatures_df.temperature_bucket, F.lit("U")))\
    .join(
        i94cntyl_df,
        F.col("i94cit") == i94cntyl_df.value
    ).join(
        countries_df,
        F.lower(i94cntyl_df.description) == F.lower(countries_df.name)
    ).select(
        "date_id",
        F.col("year"),
        countries_df.alpha3.alias("country_id"),
        airports_df.ident.alias("airport_id"),
        "gender",
        "age_bucket",
        "temperature_bucket"
    )

    return fact_ingress_df


def _validate_fact_ingress_df(df):
    """Performs simple validations on the factIngress DataFrame"""
    
    _simple_validation(
        df,
        "fact_airport_df", [
            "date_id",
            "year",
            "country_id",
            "airport_id",
            "gender",
            "age_bucket",
            "temperature_bucket"
        ],
        1000000
    )


def generate_fact_ingress_parquet(
    i94_data_df,
    airports_df,
    cbp_codes_df,
    countries_df,
    i94cntyl_df,
    temperatures_df,
    output_dir="parquet_files"
):
    """Generates factIngress parquet files"""
    fact_ingress_df = _get_fact_ingress_df(
        i94_data_df,
        airports_df,
        cbp_codes_df,
        countries_df,
        i94cntyl_df,
        temperatures_df
    )
    _validate_fact_ingress_df(fact_ingress_df)
    fact_ingress_df.write.mode("overwrite").parquet(
        f"{output_dir}/fact_ingress", partitionBy=("year", "airport_id"))
